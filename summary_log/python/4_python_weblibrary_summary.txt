----------------------------------------------------------------------------------------------------
스크래핑 <-> 크롤링
	ㄴ 스크래핑
		ㄴ 스크래핑이란 HTTP를 통해 웹 사이트의 내용을 긁어다 원하는 형태로 가공
		ㄴ 크롤링도 일종의 스크래핑
	ㄴ 크롤링
		ㄴ 크롤링은 크롤러가 하는 작업을 부르는 말로, 여러 인터넷 사이트의 페이지(문서, html 등)를 수집해서 분류
----------------------------------------------------------------------------------------------------
필요한 패키지
	ㄴ requests
		ㄴ 설치 필요
		ㄴ import requests as rq
	ㄴ urllib
		ㄴ from urllib import request
			ㄴ request에 s가 붙어있지 않음
	ㄴ bs4
		ㄴ 설치 필요
		ㄴ from bs4 import BeautifulSoup
	ㄴ json
		ㄴ import json
		ㄴ json.dumps
			ㄴ json to string
		ㄴ json.loads
			ㄴ string to json
	ㄴ selenium
		ㄴ from selenium import webdriver
		ㄴ web application testing framework
	ㄴ folium
		ㄴ 터미널 창에서..
			ㄴ pip install folium
		ㄴ python 지도 모듈
----------------------------------------------------------------------------------------------------
package - requests
	ㄴ 일반적인 import 방식
		ㄴ import requests as rq
	ㄴ requests.method
		ㄴ .get()
		ㄴ .post()
		ㄴ .put()
		ㄴ .delete()
		ㄴ .head()
		ㄴ .options()
	ㄴ .get(url)
		ㄴ url에 응답을 요청해서 response객체를 반환함
			ㄴ headers, ...
		ㄴ res = rq.get(url)
		ㄴ 파라미터 보내기
			ㄴ 1. 직접 url에 파라미터를 넣어서 보내는 방법
				url = "http://www.naver.com?a=bbb&b=123" 
				response = requests.get(url)
			ㄴ 2. 딕셔너리 이용하기
				ㄴ 사용방법
					ㄴ requests.get(url, params=...)
				ㄴ 사용예시
					paramDict = { "a" : "bbb", "b" : 123 } 
					url = "http://www.naver.com" 
					response = requests.get(url, params=paramDict)
		ㄴ SSL 인증서 사용
			ㄴ 사용방법
				ㄴ response = requests.post(url, verify=False)
		ㄴ 인증이 필요한 경우
			ㄴ 사용방법
				ㄴ response = requests.post(url, auth=("id", "pass"))
		ㄴ 그 밖의 옵션들
			ㄴ headers, cookies, timeout ...
	ㄴ urllib.request.Request()
		ㄴ 사용예시
			ㄴ request.Request(url)
		ㄴ Request객체 생성
----------------------------------------------------------------------------------------------------
package - urllib
	ㄴ 일반적인 import 방식
		ㄴ from urllib import request
	ㄴ urlopen()
		ㄴ 사용방법
			ㄴ request.urlopen(url)
		ㄴ 사용예시
			site = request.urlopen("http://www.google.com")
			page = site.read()
			header = site.getheaders()
	ㄴ urlretrieve
		ㄴ 인터넷 파일을 저장하는 함수
		ㄴ 사용방법
			ㄴ urlretrieve(url, filename)
		ㄴ 사용예시
			req.urlretrieve(url, imgName)
	ㄴ urlopen
		ㄴ urlretrieve와 달리 파일을 저장하는 것이 아닌 메모리에 로딩을 하는 함수
		ㄴ 사용방법
			ㄴ urlretrieve(url)
		ㄴ 사용예시
			site = request.urlopen(url)
			downImg = site.read()
			with open(imgName, 'wb') as f:
			    f.write(downImg)
	ㄴ urlparse
		ㄴ url을 분석하는 함수
			ㄴ scheme
			ㄴ netloc
			ㄴ path
			ㄴ params
			ㄴ query
			ㄴ fragment
	ㄴ urljoin
		ㄴ import 방식?
			ㄴ from urllib.parse import urljoin # 함수를 하나만 가져오는 경우//
		ㄴ 상대경로를 절대경로로 변환하는 함수
		ㄴ 사용방법
			ㄴ url = parse.urljoin(base, url)
				ㄴ base가 기본이 되는 url
				ㄴ url이 상대 경로(혹은 절대 경로)
		ㄴ 사용예시
			print(urljoin(baseurl, 'b.html'))
			print(urljoin(baseurl, 'sub/c.html'))
			print(urljoin(baseurl, '/sub/c.html')) #페이지의 루트부분부터 시작을 해줌
			print(urljoin(baseurl, '../sub/c.html')) #부모로 올라가서..
			print(urljoin(baseurl, 'http://www.daum.net')) #아예 통째로 변경됨
		
----------------------------------------------------------------------------------------------------
package - bs4
	ㄴ 일반적인 import 방식
		ㄴ from bs4 import BeautifulSoup
	ㄴ BeautifulSoup()
		ㄴ 사용예시
			ㄴ soup = BeautifulSoup(html, 'html.parser')
			ㄴ soup.html.body.h1
	ㄴ find_all()
		ㄴ 사용예시
			ㄴ soup.find_all('a')
		ㄴ 일반적으로 select() 메소드를 더 많이 사용하는듯
	ㄴ .attrs[]
		ㄴ 사용예시
			ㄴ links.attrs['href'] # 태그의 속성값을 가져옴
	ㄴ select, select_one
		ㄴ 사용예시
			ㄴ soup.select_one('#course > h1')
			ㄴ h1 = soup.select('#course > h1')
				ㄴ 한개의 요소를 찾아올때도 list 형태로 반환을 해줌
----------------------------------------------------------------------------------------------------
package - selenium
	ㄴ 메소드 종류
		ㄴ find_element_by_name(‘HTML_name’)
		ㄴ find_element_by_id(‘HTML_id’)
		ㄴ find_element_by_css_selector(‘#css > div.selector’)
		ㄴ find_element_by_class_name(‘some_class_name’)
		ㄴ find_element_by_tag_name(‘h1’)
		ㄴ find_element_by_xpath(‘/html/body/some/xpath’) : xpath 지정하여 해당 요소 추출
		ㄴ find_element_by_link_text(‘text’) : 링크 텍스트로 요소 추출
		ㄴ find_elemens_by_css_selector(‘#css > div.selector’)
		ㄴ find_elements_by_class_name(‘some_class_name’)
		ㄴ find_elements_by_tag_name(‘h1’)
		ㄴ clear()             : 글자를 지움
		ㄴ click()             : 클릭
		ㄴ get_attribute(name) : 요소의 속성 name에 해당하는 값을 추출
		ㄴ is_displayed()      : 요소가 화면에 출력되는지 확인
		ㄴ is_enabled()
		ㄴ is_selected()
		ㄴ save_screenshot(filename)
			ㄴ 사용예시
				ㄴ driver.save_screenshot('Website.png')
		ㄴ send_keys()
			ㄴ textfield, passwordfiled에 해당 str입력
		ㄴ submit()
		ㄴ execute_script()
	ㄴ webdriver 객체 생성
		ㄴ driver = webdriver.Chrome('./webdriver/chromedriver.exe')
		ㄴ driver.implicitly_wait(2) #2초 쉼 (드라이버가 접속할 시간을 주기 위해서)
	ㄴ 페이지 접근
		ㄴ driver.get(url)
			ㄴ 사용예시
				ㄴ driver.get('http://www.google.com')
----------------------------------------------------------------------------------------------------
package - folium
	ㄴ import folium
	ㄴ folium.Map(location= , zoom_start=)
		ㄴ folium을 이용한 지도 생성
		ㄴ 사용예시
			ㄴ map_osm = folium.Map(location=[37.572463, 126.975611], zoom_start=17)
	ㄴ folium.Marker(location=, popup=, icon=,).add_to(지도 변수이름)
		ㄴ folium 마커 생성 후 지도에 마커 표시
		ㄴ 사용예시
			ㄴ folium.Marker(location=[37.572463, 126.975611], popup="세종문화회관", icon=folium.Icon(color="red", icon="info-sign")).add_to(map_osm)
	ㄴ folium.CircleMarker()
		ㄴ folium 원형 모양 마커 생성
		ㄴ 사용예시
			ㄴ folium.CircleMarker(location=[37.572463, 126.975611], popup="광화문역", radius=100, color="red", fill_color="#AAAA33").add_to(map_osm)
	ㄴ .save(디렉토리위치)
		ㄴ folium 지도를 저장
			ㄴ 사용예시
				ㄴ map_osm.save("./map/map4.html")

----------------------------------------------------------------------------------------------------
naverlogin

사용된 document.getElementsByName('id')[0].value=\' 는 자바스크립트에서 사용되는 함수인데
파이썬에서 키를 직접적으로 넘겨주는 게 아니라 브라우져 내에서 자바스크립트로 아이디 값을 넘겨주기 때문에 네이버의 자동화된 소프트와 알고리즘을 우회하는 원리이다.
요즘 네이버는 자동로그인 같은 자동화된 소프트웨어를 막는 추세
execute_script도 언제든지 네이버의 조치에 의해 막힐 수 있다.