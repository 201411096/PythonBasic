----------------------------------------------------------------------------------------------------
스크래핑 <-> 크롤링
	ㄴ 스크래핑
		ㄴ 스크래핑이란 HTTP를 통해 웹 사이트의 내용을 긁어다 원하는 형태로 가공
		ㄴ 크롤링도 일종의 스크래핑
	ㄴ 크롤링
		ㄴ 크롤링은 크롤러가 하는 작업을 부르는 말로, 여러 인터넷 사이트의 페이지(문서, html 등)를 수집해서 분류
----------------------------------------------------------------------------------------------------
필요한 패키지
	ㄴ requests
		ㄴ 설치 필요
		ㄴ import requests as rq
	ㄴ urllib
		ㄴ from urllib import request
			ㄴ request에 s가 붙어있지 않음
	ㄴ bs4
		ㄴ 설치 필요
		ㄴ from bs4 import BeautifulSoup
	ㄴ json
		ㄴ import json
		ㄴ json.dumps
			ㄴ json to string
		ㄴ json.loads
			ㄴ string to json
	ㄴ selenium
		ㄴ from selenium import webdriver
		ㄴ web application testing framework
	ㄴ folium
		ㄴ 터미널 창에서..
			ㄴ pip install folium
		ㄴ python 지도 모듈
----------------------------------------------------------------------------------------------------
package - requests
	ㄴ 일반적인 import 방식
		ㄴ import requests as rq
	ㄴ requests.method
		ㄴ .get()
		ㄴ .post()
		ㄴ .put()
		ㄴ .delete()
		ㄴ .head()
		ㄴ .options()
	ㄴ .get(url)
		ㄴ url에 응답을 요청해서 response객체를 반환함
			ㄴ headers, ...
		ㄴ res = rq.get(url)
		ㄴ 파라미터 보내기
			ㄴ 1. 직접 url에 파라미터를 넣어서 보내는 방법
				url = "http://www.naver.com?a=bbb&b=123" 
				response = requests.get(url)
			ㄴ 2. 딕셔너리 이용하기
				ㄴ 사용방법
					ㄴ requests.get(url, params=...)
				ㄴ 사용예시
					paramDict = { "a" : "bbb", "b" : 123 } 
					url = "http://www.naver.com" 
					response = requests.get(url, params=paramDict)
		ㄴ SSL 인증서 사용
			ㄴ 사용방법
				ㄴ response = requests.post(url, verify=False)
		ㄴ 인증이 필요한 경우
			ㄴ 사용방법
				ㄴ response = requests.post(url, auth=("id", "pass"))
		ㄴ 그 밖의 옵션들
			ㄴ headers, cookies, timeout ...
----------------------------------------------------------------------------------------------------
package - urllib
	ㄴ 일반적인 import 방식
		ㄴ from urllib import request
	ㄴ urlopen()
		ㄴ 사용방법
			ㄴ request.urlopen(url)
		ㄴ 사용예시
			site = request.urlopen("http://www.google.com")
			page = site.read()
			header = site.getheaders()
	ㄴ urlretrieve
		ㄴ 인터넷 파일을 저장하는 함수
		ㄴ 사용방법
			ㄴ urlretrieve(url, filename)
		ㄴ 사용예시
			req.urlretrieve(url, imgName)
	ㄴ urlopen
		ㄴ urlretrieve와 달리 파일을 저장하는 것이 아닌 메모리에 로딩을 하는 함수
		ㄴ 사용방법
			ㄴ urlretrieve(url)
		ㄴ 사용예시
			site = request.urlopen(url)
			downImg = site.read()
			with open(imgName, 'wb') as f:
			    f.write(downImg)
	ㄴ urljoin
		ㄴ import 방식?
			ㄴ from urllib.parse import urljoin # 함수를 하나만 가져오는 경우//
		ㄴ 상대경로를 절대경로로 변환하는 함수
		ㄴ 사용예시
			print(urljoin(baseurl, 'b.html'))
			print(urljoin(baseurl, 'sub/c.html'))
			print(urljoin(baseurl, '/sub/c.html')) #페이지의 루트부분부터 시작을 해줌
			print(urljoin(baseurl, '../sub/c.html')) #부모로 올라가서..
			print(urljoin(baseurl, 'http://www.daum.net')) #아예 통째로 변경됨
		
----------------------------------------------------------------------------------------------------
naverlogin

사용된 document.getElementsByName('id')[0].value=\' 는 자바스크립트에서 사용되는 함수인데
파이썬에서 키를 직접적으로 넘겨주는 게 아니라 브라우져 내에서 자바스크립트로 아이디 값을 넘겨주기 때문에 네이버의 자동화된 소프트와 알고리즘을 우회하는 원리이다.
요즘 네이버는 자동로그인 같은 자동화된 소프트웨어를 막는 추세
execute_script도 언제든지 네이버의 조치에 의해 막힐 수 있다.