0624_수요일

	수집	->	저장	->	처리	->	분석	->	시각화
----------------------------------------------------------------------------------------------------
jsoup			hadoop		hadoop		python		python
beautifulsoup		RDBMS		pig		R		R
flume					hive
kafka
logstash

* hadoop = hadoop(hdfs) + hadoop(MapReduce) + yarn(Resource Manage)
	ㄴ 저장
		ㄴ hadoop(hdfs)
	ㄴ 처리
		ㄴ hadoop(mapreduce)
* flume = source -> channel -> sink
	ㄴ 실습 때는 sink를 hadoop으로 맞춤 (agent)
		ㄴ 여러개의 agent로 구성될 수도 있음
	
elk : logstash - elastic search - kibana

데이터 참고 : kaggle
----------------------------------------------------------------------------------------------------
hive
	ㄴ 파일로 되어있는 데이터들을 db에 있는 테이블 구조처럼 데이터를 가져옴
	ㄴ 정형화된 데이터를 관리하는데 용이함
	ㄴ metastore
		ㄴ 테이블 구조와 데이터 위치를 mysql에 ...
	ㄴ 관리자가 설치하고 사용권한을 넘겨주는 방식
----------------------------------------------------------------------------------------------------
mariadb 설치

[root@dn01 ~]# yum install mariadb-server mariadb
[root@dn01 ~]# rpm -qa | grep maria
[root@dn01 ~]# systemctl enable mariadb.service
[root@dn01 ~]# systemctl start mariadb.service
[root@dn01 ~]# mysql_secure_installation
-----
Enter current password for root (enter for none):
OK, successfully used password, moving on...

Setting the root password ensures that nobody can log into the MariaDB
root user without the proper authorisation.

Set root password? [Y/n] Y
New password:
Re-enter new password:
Password updated successfully!
Reloading privilege tables..
 ... Success!


By default, a MariaDB installation has an anonymous user, allowing anyone
to log into MariaDB without having to have a user account created for
them.  This is intended only for testing, and to make the installation
go a bit smoother.  You should remove them before moving into a
production environment.

Remove anonymous users? [Y/n] n
 ... skipping.

Normally, root should only be allowed to connect from 'localhost'.  This
ensures that someone cannot guess at the root password from the network.

Disallow root login remotely? [Y/n] n
 ... skipping.

By default, MariaDB comes with a database named 'test' that anyone can
access.  This is also intended only for testing, and should be removed
before moving into a production environment.

Remove test database and access to it? [Y/n] n
 ... skipping.

Reloading the privilege tables will ensure that all changes made so far
will take effect immediately.

Reload privilege tables now? [Y/n] Y
 ... Success!

Cleaning up...

All done!  If you've completed all of the above steps, your MariaDB
installation should now be secure.

Thanks for using MariaDB!

-----
입력값 요약
	enter -> Y -> admin1234 -> admin1234 -> n -> n -> n -> Y
-----
[root@dn01 ~]# vi /etc/my.cnf
	bind-address=192.168.56.102
	[mysqld_safe] 바로 윗단에... 수정..
[root@dn01 ~]# systemctl restart mariadb.service
[root@dn01 ~]# mysql -u root -p
	ㄴ admin1234 입력 후 확인
MariaDB [(none)]> show databases;
MariaDB [(none)]> grant all privileges on *.* to hive@"%" identified by "hive" with grant option;
MariaDB [(none)]> flush privileges;
MariaDB [(none)]> grant all privileges on *.* to hive@"dn01" identified by "hive" with grant option;
MariaDB [(none)]> flush privileges;
MariaDB [(none)]> use mysql;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
MariaDB [mysql]> select user, host from user;
+------+-----------+
| user | host      |
+------+-----------+
| hive | %         |
| root | 127.0.0.1 |
| root | ::1       |
|      | dn01      |
| hive | dn01      |
| root | dn01      |
|      | localhost |
| root | localhost |
+------+-----------+
8 rows in set (0.00 sec)

MariaDB [mysql]>exit
Bye
----------------------------------------------------------------------------------------------------
하이브 설치
[ dn01 노드의  root 계정에서 ]
1. Hive 설치 ( 버전이 계속 변경된다 )
[root@dn01 ~]# cd /tmp
[root@dn01 ~]# wget http://apache.mirror.cdnetworks.com/hive/hive-2.3.7/apache-hive-2.3.7-bin.tar.gz
[root@dn01 ~]# tar xzvf apache-hive-2.3.7-bin.tar.gz
[root@dn01 ~]# mkdir -p /opt/hive/2.3.7
[root@dn01 ~]# mv apache-hive-2.3.7-bin/* /opt/hive/2.3.7/
[root@dn01 ~]# ln -s /opt/hive/2.3.7 /opt/hive/current
[root@dn01 ~]# cd /opt/hive/current/

2. Hive 폴더 접근 권한 설정
[root@dn01 ~]# chmod -R 775 /opt/hive/2.3.7/
[root@dn01 ~]# chown -R hadoop:hadoop /opt/hive/

3. Hadoop 계정으로 전환
[root@dn01 ~]# su - hadoop

4. HIVE 환경 변수 추가
[hadoop@dn01 ~]# vi ~/.bash_profile

        #### HIVE 2.3.7 #######################
           export HIVE_HOME=/opt/hive/current
           export PATH=$PATH:$HIVE_HOME/bin
           export CLASSPATH=.:${JAVA_HOME}/lib:${JREHOME}/lib:/opt/hive/current/lib
        #### HIVE 2.3.7 #######################

[hadoop@dn01 ~]# source .bash_profile

5. HIVE 설정 파일 복사
[hadoop@dn01 ~]# cp /opt/hive/current/conf/hive-env.sh.template /opt/hive/current/conf/hive-env.sh
[hadoop@dn01 ~]# cp /opt/hive/current/conf/hive-default.xml.template /opt/hive/current/conf/hive-site.xml

6. HIVE 설정 파일 수정 ( 해당 부분을 찾아서 수정 - 제발 오타 확인!!!!!  )
[hadoop@dn01 ~]# vi /opt/hive/current/conf/hive-env.sh
HADOOP_HOME=/opt/hadoop/current

vi 명령모드에서.. "/ConnectionURL" 이런식으로 찾으면됨

<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://192.168.56.102:3306/hive?createDatabaseIfNotExist=true</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>
	ㄴ value값만 수정함 ****************************************
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
  <description>username to use against metastore database</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
  <description>password to use against metastore database</description>
</property>
	ㄴ 529라인으로 가야됨 ***********************************************
 <property>
 <name>hive.exec.local.scratchdir</name>
 <value>/home/hadoop/iotmp</value>
 <description>Local scratch space for Hive jobs</description>
 </property>
	ㄴ 74라인으로 감
 <property>
 <name>hive.downloaded.resources.dir</name>
 <value>/home/hadoop/iotmp</value>
 <description>Temporary local directory for added resources in the remote file system.</description>
 </property>
<property>
    <name>hive.cli.print.current.db</name>
    <value>true</value>
    <description>Whether to include the current database in the Hive prompt.</description>
</property>

7. HIVE 관련 디렉토리 생성 및 권한 변경
[hadoop@dn01 ~]# mkdir -p /home/hadoop/iotmp
[hadoop@dn01 ~]# chmod -R 775 /home/hadoop/iotmp/

8. MYSQL Connector 다운로드 및 hive lib로 복사
[hadoop@dn01 ~]# cd /tmp
[hadoop@dn01 tmp]# wget http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.38.tar.gz
[hadoop@dn01 tmp]# tar xzvf mysql-connector-java-5.1.38.tar.gz
[hadoop@dn01 tmp]# cd mysql-connector-java-5.1.38
[hadoop@dn01 mysql-connector-java-5.1.38]# mv mysql-connector-java-5.1.38-bin.jar /opt/hive/current/lib/

9. HIVE 기본 디렉터리 생성 및 권한 추가
[hadoop@dn01 ~]# hdfs dfs -mkdir /tmp
[hadoop@dn01 ~]# hdfs dfs -mkdir -p /user/hive/warehouse
[hadoop@dn01 ~]# hdfs dfs -chmod -R 777 /tmp
[hadoop@dn01 ~]# hdfs dfs -chmod -R 777 /user/hive/warehouse

10. HIVE mysql 기본 스키마 생성
[hadoop@dn01 ~]# schematool -initSchema -dbType mysql
( 에러가 발생한다면 기존에 같은 이름의 데이타베이스가 있으니깐
  show databases에서 drop database hive; 제거 )

11. HIVE 접속
[hadoop@dn01 ~]# hive
브라우저에서 http://192.168.56.101:50070
메뉴 >  Utitlies > Browser Directory >
 /   
user
hive
warehouse 에서 앞으로 확인

12. beeline  접속하기 위한 추가 작업
     beeline은 그룹과 유저가 other이기 때문에

****** nn01의 hadoop계정에서 진행

* 모든 노드의 core-site.xml 에 수정 :
모든 그룹과 호스트에게 접속하기 위한 관문역할의  proxy를 모두가 가능하도록 변경
cd $HADOOP_HOME/etc/hadoop
vi core-site.xml

<property>
  <name>hadoop.proxyuser.hadoop.groups</name>
  <value>*</value>
</property>
<property>
  <name>hadoop.proxyuser.hadoop.hosts</name>
  <value>*</value>
</property>

* beeline은 others 권한으로 접속하므로, HDFS 권한을 수정해준다.
hdfs dfs -chmod -R 777 /tmp
hdfs dfs -chmod -R 777 /user/hive/warehouse

core-site.xml 수정하여 dn01, dn02에 복사.
scp core-site.xml hadoop@dn01:/opt/hadoop/current/etc/hadoop
scp core-site.xml hadoop@dn02:/opt/hadoop/current/etc/hadoop

----------------------------------------------------------------------------------------------------
mariadb 상태 확인
[hadoop@dn01 ~]$ systemctl status mariadb.service
----------------------------------------------------------------------------------------------------
[hadoop@dn01 ~]$ hive --service metastore &
	ㄴ enter 치면 됨
[hadoop@dn01 ~]$ ps -ef | grep metastore
[hadoop@dn01 ~]$ hive --service hiveserver2 &
	ㄴ enter 치면 됨
[hadoop@dn01 ~]$ ps -ef | grep hiveserver2

hive (default)> show databases;
OK
default
Time taken: 5.894 seconds, Fetched: 1 row(s)
hive (default)> create database sample1;
OK
Time taken: 0.154 seconds
hive (default)> show databases;
OK
default
sample1
Time taken: 0.019 seconds, Fetched: 2 row(s)
hive (default)> use sample1;
OK
Time taken: 0.023 seconds
hive (sample1)> create table emp(name string);
OK
Time taken: 0.469 seconds
hive (sample1)> insert into emp(name) values('KIM');
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = hadoop_20200624030212_2bd65b0e-8063-4fd4-936e-2eaa7b0e4031
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1592958835330_0001, Tracking URL = http://nn01:8088/proxy/application_1592958835330_0001/
Kill Command = /opt/hadoop/current/bin/hadoop job  -kill job_1592958835330_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2020-06-24 03:02:28,817 Stage-1 map = 0%,  reduce = 0%
2020-06-24 03:02:38,800 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec
MapReduce Total cumulative CPU time: 990 msec
Ended Job = job_1592958835330_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://nn01:9000/user/hive/warehouse/sample1.db/emp/.hive-staging_hive_2020-06-24_03-02-12_806_167839477531097723-1/-ext-10000
Loading data to table sample1.emp
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 0.99 sec   HDFS Read: 3757 HDFS Write: 71 SUCCESS
Total MapReduce CPU Time Spent: 990 msec
OK
Time taken: 27.789 seconds
hive (sample1)>

--dn02에서 확인..
[root@dn02 ~]# ssh dn01
The authenticity of host 'dn01 (192.168.56.102)' can't be established.
ECDSA key fingerprint is SHA256:Jb0VaQ3QBJATHVFbGOaC6V3ZpY26bQRxcXEwhHm9jt8.
ECDSA key fingerprint is MD5:05:40:37:4b:20:a2:18:5c:dc:41:a4:c4:02:a2:1c:03.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'dn01,192.168.56.102' (ECDSA) to the list of known hosts.
root@dn01's password:
Last login: Wed Jun 24 00:32:18 2020 from 192.168.56.1
[root@dn01 ~]# hdfs dfs -ls /user/hive/warehouse/sample1.db/emp
Found 1 items
-rwxrwxrwx   1 hadoop supergroup          4 2020-06-24 03:02 /user/hive/warehouse/sample1.db/emp/000000_0
[root@dn01 ~]# hdfs dfs -cat /user/hive/warehouse/sample1.db/emp
cat: `/user/hive/warehouse/sample1.db/emp': Is a directory
[root@dn01 ~]# hdfs dfs -cat /user/hive/warehouse/sample1.db/emp/000000_0
KIM
[root@dn01 ~]#
--다시 dn01에서..
[hadoop@dn01 ~]$ beeline
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hive/2.3.7/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop/2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Beeline version 2.3.7 by Apache Hive
beeline> !connect jdbc:hive2://
Connecting to jdbc:hive2://
Enter username for jdbc:hive2://: hive
Enter password for jdbc:hive2://: ****
20/06/24 03:14:29 [main]: WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
Connected to: Apache Hive (version 2.3.7)
Driver: Hive JDBC (version 2.3.7)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://> show databases;
OK
+----------------+
| database_name  |
+----------------+
| default        |
| sample1        |
+----------------+
2 rows selected (1.452 seconds)
0: jdbc:hive2://> use sample1
. . . . . . . . > ;
OK
No rows affected (0.193 seconds)
0: jdbc:hive2://> insert into emp(name) values('park');
....
0: jdbc:hive2://> select * from emp;
OK
+-----------+
| emp.name  |
+-----------+
| KIM       |
| park      |
+-----------+
2 rows selected (0.276 seconds)
0: jdbc:hive2://> !quit



